{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'VarianceThresholdSelector' from 'pyspark.ml.feature' (D:\\spark\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\ml\\feature.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ebe5a3391871>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringIndexer\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mStandardScaler\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mImputer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mChiSqSelector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVarianceThresholdSelector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'VarianceThresholdSelector' from 'pyspark.ml.feature' (D:\\spark\\spark-3.0.1-bin-hadoop2.7\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\ml\\feature.py)"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.ml.feature import StringIndexer , StandardScaler , Imputer, OneHotEncoder, VectorAssembler, ChiSqSelector, VarianceThresholdSelector\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.ml import Pipeline,  PipelineModel\n",
    "\n",
    "import pickle\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkContext \n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark\n",
    "\n",
    "import json\n",
    "from pyspark.sql.column import Column\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part1 - load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Data Lake\n",
    "\n",
    "def get_data_spark(hive_statement):\n",
    "    \"\"\"Imports data from PV Cluster - contact Asia_Data_Lab@manulife.com for access\n",
    "\t\n",
    "\tArgs:\n",
    "\t\n",
    "\t1. hive_statement = select query from tables in PV cluster\n",
    "    \n",
    "    Returns: Pandas Dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df_spark = hive.executeQuery(hive_statement)\n",
    "\n",
    "    return df_spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From CSV\n",
    "\n",
    "def get_data_csv(path, csv_file):\n",
    "    \"\"\"Imports data from CSV file\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    1. path = file folder\n",
    "    2. csv_file = name of csv file\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Spark Dataframe\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    df_spark = spark.read.csv(path+csv_file, inferSchema=True, header=True)\n",
    "    \n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------+----+--------------+--------+-------------------+----------------------+-----------------+\n",
      "|Case_No|Age_Mons|Qchat-10-Score| Sex|     Ethnicity|Jaundice|Family_mem_with_ASD|Who completed the test|Class/ASD Traits |\n",
      "+-------+--------+--------------+----+--------------+--------+-------------------+----------------------+-----------------+\n",
      "|      1|    null|             3|null|middle eastern|     yes|                 no|         family member|               No|\n",
      "|      5|    null|             9|null|White European|      no|                yes|         family member|              Yes|\n",
      "|     13|    null|             0|null|middle eastern|     yes|                 no|         family member|               No|\n",
      "|     14|    null|             7|null|middle eastern|     yes|                 no|         family member|              Yes|\n",
      "|     18|    null|             8|null|middle eastern|     yes|                 no|         family member|              Yes|\n",
      "+-------+--------+--------------+----+--------------+--------+-------------------+----------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/admin/AppData/Roaming/Microsoft/Windows/Start Menu/Programs/udemy/Datasets/\"\n",
    "\n",
    "df_spark = get_data_csv(path = \"C:/Users/admin/AppData/Roaming/Microsoft/Windows/Start Menu/Programs/udemy/Datasets/\",\n",
    "                       csv_file  = 'Toddler Autism dataset July 2018_practice.csv')\n",
    "\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(df, remove_list):\n",
    "    \"\"\"Removes uncesscary columms and creates feature dataframe\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    df : spark dataframe input_data\n",
    "    remove_list = list object containing columns to drop\n",
    "    \n",
    "    Returns: new dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.drop(*remove_list)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Case_No: integer (nullable = true)\n",
      " |-- Age_Mons: integer (nullable = true)\n",
      " |-- Qchat-10-Score: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Ethnicity: string (nullable = true)\n",
      " |-- Jaundice: string (nullable = true)\n",
      " |-- Family_mem_with_ASD: string (nullable = true)\n",
      " |-- Who completed the test: string (nullable = true)\n",
      " |-- Class/ASD Traits : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Case_No: integer (nullable = true)\n",
      " |-- Age_Mons: integer (nullable = true)\n",
      " |-- Qchat-10-Score: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Jaundice: string (nullable = true)\n",
      " |-- Family_mem_with_ASD: string (nullable = true)\n",
      " |-- Class/ASD Traits : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = data_preparation(df_spark, remove_list = ['Ethnicity', 'Who completed the test'])\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3 : Train Tesy split. Pyspark does not have stratified train and test split. Use this custom function instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def stratified_split_train_test(df, frac, label, join_on, seed=123):\n",
    "    \"\"\"\n",
    "    Creates  a stratified train test split for the dataset. Stratification is based on % of Y\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    1. df = input spark dataframe\n",
    "    2. frac = fraction of training set to be used\n",
    "    3. label = target variable\n",
    "    4. join_on =   unique key to ensure uniqueness of train and test data\n",
    "    \n",
    "    Returns \n",
    "    \n",
    "    df_train = df based on fraction\n",
    "    df_test =  % remaining from fraction\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    fractions = df.select(label).distinct().withColumn(\"fraction\", F.lit(frac)).rdd.collectAsMap()\n",
    "    df_train = df.stat.sampleBy(label, fractions, seed)\n",
    "    df_test = df.join(df_train, on=join_on, how=\"left_anti\")\n",
    "    \n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = stratified_split_train_test(df_spark, frac= 0.7, label = 'Class/ASD Traits ' , join_on = 'Case_No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Class/ASD Traits |count|\n",
      "+-----------------+-----+\n",
      "|               No|  326|\n",
      "|              Yes|  728|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupby('Class/ASD Traits ').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Class/ASD Traits |count|\n",
      "+-----------------+-----+\n",
      "|               No|  213|\n",
      "|              Yes|  514|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.groupby('Class/ASD Traits ').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Class/ASD Traits |count|\n",
      "+-----------------+-----+\n",
      "|               No|  113|\n",
      "|              Yes|  214|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.groupby('Class/ASD Traits ').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4  : Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meta(col, metadata):\n",
    "            meta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(json.dumps(metadata))\n",
    "            return Column(getattr(col._jc, \"as\")('', meta))\n",
    "\n",
    "def feature_engineering(x_train, x_test, unique_key, target_var , use_scaling = True):\n",
    "    \"\"\"\n",
    "    Create Feature Engineering Pipeline \n",
    "    \n",
    "    Apply encoding to categorical variables\n",
    "    Imputation of null to both numeric and categorical variables\n",
    "    \n",
    "    \n",
    "    Args : \n",
    "    \n",
    "    1. x_train, x_test  = spark dataframe\n",
    "    2. unique_key = unique key of dataset\n",
    "    3. target_var = target variable\n",
    "    4. use_scaling = if True, Standardize, otherwise use original scale.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    x_train,\n",
    "    x_test,\n",
    "    preprocessing_pipe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    categorical_columns = [item[0] for item in x_train.dtypes if item[1].startswith('string')  and item[0] != unique_key and item[0] != target_var]\n",
    "    numerical_columns = [item[0] for item in x_train.dtypes if not item[1].startswith('string') and item[0] != unique_key and item[0] != target_var]\n",
    " \n",
    "\n",
    "    x_train = x_train.fillna(0, subset=numerical_columns)\n",
    "    x_train = x_train.fillna(\"Missing\" ,subset = categorical_columns)\n",
    "    x_test  = x_test.fillna(0, subset=numerical_columns)\n",
    "    x_test = x_test.fillna(\"Missing\" ,subset = categorical_columns)\n",
    "    \n",
    "    \n",
    "    #Define Steps in Pipelines: \n",
    "    \n",
    "    indexer = StringIndexer(inputCols=categorical_columns, outputCols=[c+\"_index\" for c in categorical_columns])\n",
    "\n",
    "    onehot  = OneHotEncoder(dropLast=True,\n",
    "                        inputCols=[c for c in indexer.getOutputCols()],\n",
    "                        outputCols=[c+\"_onehot\" for c in indexer.getOutputCols()])\n",
    "\n",
    "    catvec  = VectorAssembler(inputCols=onehot.getOutputCols(), outputCol='cat_features')\n",
    "\n",
    "    numvec   = VectorAssembler(inputCols=numerical_columns, outputCol='num_features')\n",
    "    \n",
    "    Standard = StandardScaler(inputCol=\"num_features\", outputCol=\"num_features_scaled\")\n",
    "\n",
    "    \n",
    "    #feature_vector_scaled = VectorAssembler(inputCols=[\"cat_features\", \"num_features_m\"], outputCol=\"features\")\n",
    "    #feature_vector = VectorAssembler(inputCols=[\"cat_features\", \"num_features\"], outputCol=\"features\")\n",
    "    \n",
    "    \n",
    "    #Define Pipelines \n",
    "    \n",
    "    categorical_pipeline = Pipeline(stages=[indexer, onehot, catvec])\n",
    "    \n",
    "\n",
    "    if use_scaling:\n",
    "        \n",
    "            #Function to Get Metadata from StandardScaler\n",
    "\n",
    "      \n",
    "        numerical_pipeline = Pipeline(stages =[numvec, Standard])\n",
    "        preprocessing_pipe = Pipeline(stages=[categorical_pipeline, numerical_pipeline])\n",
    "        \n",
    "        \n",
    "        pipeline_preprocess = preprocessing_pipe.fit(x_train)\n",
    "        \n",
    "        #pipeline_preprocess.save(\"C:/Users\")\n",
    "        \n",
    "\n",
    "        x_train = preprocessing_pipe.fit(x_train).transform(x_train)\n",
    "        \n",
    "    \n",
    "        x_test = pipeline_preprocess.transform(x_test)\n",
    "    \n",
    "        #pickle.dump(preprocessing_pipe, open(trans_pipe, 'wb'))\n",
    "        \n",
    "        x_train =  x_train.withColumn('num_features_mt',\n",
    "                                add_meta(x_train.num_features_scaled, \n",
    "                                metadata=x_train.schema[\"num_features\"].metadata))\n",
    "        x_test =   x_test.withColumn('num_features_mt',\n",
    "                                add_meta(x_test.num_features_scaled, \n",
    "                                metadata=x_test.schema[\"num_features\"].metadata))\n",
    "        \n",
    "        return x_train, x_test ,pipeline_preprocess\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        numerical_pipeline = Pipeline(stages =[numvec])\n",
    "        \n",
    "        preprocessing_pipe = Pipeline(stages=[categorical_pipeline, numerical_pipeline])\n",
    "    \n",
    "    \n",
    "        pipeline_preprocess = preprocessing_pipe.fit(x_train)\n",
    "        \n",
    "       # pipeline_preprocess.save(\"C:/Users\")\n",
    "    \n",
    "        x_train = preprocessing_pipe.fit(x_train).transform(x_train)\n",
    "    \n",
    "        x_test = pipeline_preprocess.transform(x_test)\n",
    "    \n",
    "        #pickle.dump(preprocessing_pipe, open(trans_pipe, 'wb'))\n",
    "    \n",
    "        return x_train, x_test ,pipeline_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, pipeline_preprocess =feature_engineering(df_train, df_test, unique_key = 'Case_No', target_var =  'Class/ASD Traits ', use_scaling = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Class/ASD Traits |count|\n",
      "+-----------------+-----+\n",
      "|               No|  213|\n",
      "|              Yes|  514|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train.groupby('Class/ASD Traits ').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Class/ASD Traits |count|\n",
      "+-----------------+-----+\n",
      "|               No|  113|\n",
      "|              Yes|  214|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_test.groupby('Class/ASD Traits ').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final VectorAssembler \n",
    "\n",
    "def feature_vector(df_train, df_test,inputCols, outputCol):\n",
    "    \"\"\"VectorAssembler\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    df_train\n",
    "    df_test\n",
    "    inputCols = list of columns\n",
    "    outputCols = new feature names\n",
    "    \n",
    "    Returns\n",
    "    \n",
    "    df_train, df_test \n",
    "    \"\"\"\n",
    "    \n",
    "    feature_vector = VectorAssembler(inputCols=inputCols, outputCol=outputCol)\n",
    "        \n",
    "    x_train = feature_vector.transform(df_train)\n",
    "    x_test = feature_vector.transform(df_test)\n",
    "    \n",
    "    \n",
    "    return x_train, x_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = feature_vector(x_train, x_test, inputCols = [\"cat_features\", \"num_features\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case_No</th>\n",
       "      <th>Age_Mons</th>\n",
       "      <th>Qchat-10-Score</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Jaundice</th>\n",
       "      <th>Family_mem_with_ASD</th>\n",
       "      <th>Class/ASD Traits</th>\n",
       "      <th>Sex_index</th>\n",
       "      <th>Jaundice_index</th>\n",
       "      <th>Family_mem_with_ASD_index</th>\n",
       "      <th>Sex_index_onehot</th>\n",
       "      <th>Jaundice_index_onehot</th>\n",
       "      <th>Family_mem_with_ASD_index_onehot</th>\n",
       "      <th>cat_features</th>\n",
       "      <th>num_features</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>(0.0)</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case_No  Age_Mons  Qchat-10-Score      Sex Jaundice Family_mem_with_ASD  \\\n",
       "0       13         0               0  Missing      yes                  no   \n",
       "\n",
       "  Class/ASD Traits   Sex_index  Jaundice_index  Family_mem_with_ASD_index  \\\n",
       "0                No        2.0             1.0                        0.0   \n",
       "\n",
       "  Sex_index_onehot Jaundice_index_onehot Family_mem_with_ASD_index_onehot  \\\n",
       "0       (0.0, 0.0)                 (0.0)                            (1.0)   \n",
       "\n",
       "           cat_features num_features                        features  \n",
       "0  (0.0, 0.0, 0.0, 1.0)   (0.0, 0.0)  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0)  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case_No</th>\n",
       "      <th>Age_Mons</th>\n",
       "      <th>Qchat-10-Score</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Jaundice</th>\n",
       "      <th>Family_mem_with_ASD</th>\n",
       "      <th>Class/ASD Traits</th>\n",
       "      <th>Sex_index</th>\n",
       "      <th>Jaundice_index</th>\n",
       "      <th>Family_mem_with_ASD_index</th>\n",
       "      <th>Sex_index_onehot</th>\n",
       "      <th>Jaundice_index_onehot</th>\n",
       "      <th>Family_mem_with_ASD_index_onehot</th>\n",
       "      <th>cat_features</th>\n",
       "      <th>num_features</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Missing</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0)</td>\n",
       "      <td>(0.0)</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>[0.0, 3.0]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 3.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case_No  Age_Mons  Qchat-10-Score      Sex Jaundice Family_mem_with_ASD  \\\n",
       "0        1         0               3  Missing      yes                  no   \n",
       "\n",
       "  Class/ASD Traits   Sex_index  Jaundice_index  Family_mem_with_ASD_index  \\\n",
       "0                No        2.0             1.0                        0.0   \n",
       "\n",
       "  Sex_index_onehot Jaundice_index_onehot Family_mem_with_ASD_index_onehot  \\\n",
       "0       (0.0, 0.0)                 (0.0)                            (1.0)   \n",
       "\n",
       "           cat_features num_features                        features  \n",
       "0  (0.0, 0.0, 0.0, 1.0)   [0.0, 3.0]  (0.0, 0.0, 0.0, 1.0, 0.0, 3.0)  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ml_attr': {'attrs': {'numeric': [{'idx': 4, 'name': 'num_features_Age_Mons'},\n",
       "    {'idx': 5, 'name': 'num_features_Qchat-10-Score'}],\n",
       "   'binary': [{'idx': 0, 'name': 'cat_features_Sex_index_onehot_m'},\n",
       "    {'idx': 1, 'name': 'cat_features_Sex_index_onehot_f'},\n",
       "    {'idx': 2, 'name': 'cat_features_Jaundice_index_onehot_no'},\n",
       "    {'idx': 3, 'name': 'cat_features_Family_mem_with_ASD_index_onehot_no'}]},\n",
       "  'num_attrs': 6}}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.schema['features'].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ml_attr': {'attrs': {'numeric': [{'idx': 4, 'name': 'num_features_Age_Mons'},\n",
       "    {'idx': 5, 'name': 'num_features_Qchat-10-Score'}],\n",
       "   'binary': [{'idx': 0, 'name': 'cat_features_Sex_index_onehot_m'},\n",
       "    {'idx': 1, 'name': 'cat_features_Sex_index_onehot_f'},\n",
       "    {'idx': 2, 'name': 'cat_features_Jaundice_index_onehot_no'},\n",
       "    {'idx': 3, 'name': 'cat_features_Family_mem_with_ASD_index_onehot_no'}]},\n",
       "  'num_attrs': 6}}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.schema['features'].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5 : Feature Selection\n",
    "\n",
    "Variance Treshold and ChiSq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VarianceThresholdSelector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-ea66860b8000>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVarianceThresholdSelector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvarianceThreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"selectedFeatures\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VarianceThresholdSelector' is not defined"
     ]
    }
   ],
   "source": [
    "selector = VarianceThresholdSelector(varianceThreshold=8.0, outputCol=\"selectedFeatures\")\n",
    "\n",
    "result = selector.fit(x_train).transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
